# 一句提示词，Gemini 3.1 高效实现PINN求解高频PDE——与Claude Sonnet 4.6的正面交锋

*一位计算数学方向科研人员的实测手记*

---

> **核心结论**：同一句提示词让Gemini 3.1和Claude Sonnet 4.6各自实现PINNs求解高频热方程（$k=5$），Gemini主动引入傅里叶特征映射克服频谱偏置、采用加权损失平衡多任务训练、以固定数据配合L-BFGS保障二阶收敛——**4分钟达到L2相对误差3.93×10⁻⁴**；Claude使用标准MLP加等权重损失，L-BFGS阶段因随机重采样丧失收敛性——**35分钟达到4.83×10⁻⁴**。训练效率相差近9倍，精度Gemini略优。Claude在代码工程规范性和输出完整性上更胜一筹，但在算法选择的"学术直觉"上，Gemini展现出对PINN研究领域更深的理解。两份代码均不存在精确解泄露等作弊行为，对比公正可信。

---

最近我一直在关注大模型在科学计算领域的代码生成能力。恰逢2026年2月这个"神仙打架"的时间窗口——2月17日Anthropic发布了Claude Sonnet 4.6，号称以Sonnet价格逼近Opus性能，开发者偏好率高达70%；仅两天后的2月19日，Google就推出了Gemini 3.1 Pro，在Humanity's Last Exam上拿到44.4%（超过GPT-5.2的34.5%），ARC-AGI-2逻辑推理直接从上一代的31.1%翻倍到77.1%。两家都宣称自己在推理和编程上取得了突破。

但基准测试终归是基准测试。我更想知道的是：在我每天打交道的科研场景里——比如用物理信息神经网络（PINNs）求解偏微分方程——这两个模型的表现到底怎么样？于是我做了一个简单直接的测试：用**同一句提示词**，让两个模型各自完成一个完整的PINN实现。

我的提示词是这样的：

> 给出PINNs求解高频heat方程的正问题完整代码，注意解析解用于计算L2测试相对误差，注意打印各种loss，保存各种数据txt，图像。
> 1，要求pytorch-GPU训练，adam2万次，lbfgs1万次
> 2，要求精度至少1e-2
> 3，返回py文件

提示词不长，但信息密度不低。它要求模型理解什么是"高频heat方程的正问题"，给出解析解，完成Adam+L-BFGS两阶段训练，还要处理loss记录、数据保存和可视化。这是一个典型的PINN科研任务，对模型的数学理解力、编程能力和工程完整性都有考验。

两个模型都一次性返回了可运行的py文件，没有报错。但当我仔细看代码、跑完实验之后，差距远比我预想的要大。

---

## 对"高频"二字的理解，高下立判

两个模型都选择了同一个经典构型：热传导方程 $u_t - \alpha u_{xx} = 0$，热扩散系数 $\alpha=0.01$，空间频率 $k=5$，解析解为 $u(x,t) = \sin(5\pi x)\exp(-0.01(5\pi)^2 t)$。对正问题的基本认识是一致的，这没什么可说的。

真正让我眼前一亮的是Gemini 3.1对网络架构的选择。面对"高频"这个关键词，Gemini没有使用标准的全连接网络，而是主动引入了**傅里叶特征映射**（Fourier Feature Mapping）：

```python
class FourierFeaturePINN(nn.Module):
    def __init__(self, in_dim, out_dim, hidden_layers, sigma=3.0):
        super(FourierFeaturePINN, self).__init__()
        self.B = nn.Parameter(torch.randn(in_dim, hidden_layers[0] // 2) * sigma,
                              requires_grad=False)
        # ...隐藏层构建...

    def forward(self, x):
        x_proj = 2.0 * np.pi * torch.matmul(x, self.B)
        x = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
        for layer in self.layers:
            x = self.activation(layer(x))
        x = self.out_layer(x)
        return x
```

做PINN的人应该很熟悉这个思路。Tancik等人在NeurIPS 2020上发表的那篇"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains"指出，标准MLP存在严重的**频谱偏置**（spectral bias）——网络倾向于先学低频成分，对高频信号的拟合效率很低。傅里叶特征映射通过在输入端施加随机傅里叶基展开，将低维输入投射到高维频率空间，从根本上缓解了这个问题。对于空间频率 $k=5$ 的高频热方程，这一技术选择可以说正中要害。

而Claude Sonnet 4.6呢？它给出的是一个中规中矩的标准MLP：

```python
class PINN(nn.Module):
    def __init__(self):
        super().__init__()
        layers = [nn.Linear(2, HIDDEN_DIM), nn.Tanh()]
        for _ in range(N_LAYERS - 1):
            layers += [nn.Linear(HIDDEN_DIM, HIDDEN_DIM), nn.Tanh()]
        layers.append(nn.Linear(HIDDEN_DIM, 1))
        self.net = nn.Sequential(*layers)
```

6层64宽的Tanh网络，21057个参数，Xavier初始化。从软件工程角度挑不出毛病——代码整洁，注释完备，甚至还贴心地加了权重初始化的独立方法。但从算法层面看，面对一个明确标注"高频"的PDE问题，它没有做任何针对性的架构适配。它交出的是一份合格的PINN入门教程代码，而不是一份针对高频场景的解决方案。

这个差异令我印象深刻。Gemini准确地抓住了提示词中"高频"这个关键限定词，并将其映射到了正确的技术方案上。这不只是编程能力的差别，更像是对PINN研究领域知识图谱的深度理解差异。

---

## 损失函数设计：一个容易被忽略的关键细节

PINNs本质上是一个多任务学习问题——PDE残差、初始条件、边界条件三项损失需要同时优化。这三项的量级和收敛速度往往差距很大，如何平衡它们的权重是PINN实践中的核心技巧之一。

Gemini 3.1给出的损失函数是这样的：

```python
loss_total = loss_pde + 20.0 * loss_ic + 20.0 * loss_bc
```

它对初始条件和边界条件各施加了20倍的权重。这是一个深思熟虑的选择——对于热方程正问题，PDE残差（涉及二阶导数）的量级通常远大于IC/BC的均方误差，等权重很容易导致边界条件欠拟合，进而影响全局精度。PINN文献中关于这一问题的讨论由来已久，从Wang et al. (2021) 的NTK视角自适应加权，到McClenny & Brader (2023) 的self-adaptive weights，核心出发点都是一样的：**边界和初始条件需要加权强化**。

相比之下，Claude的损失函数是朴素的等权重求和：

```python
return loss_pde + loss_ic + loss_bc, loss_pde, loss_ic, loss_bc
```

没有任何加权处理。从训练日志来看，Claude的Adam阶段在5000步左右IC loss就已经降到了1e-6量级，而PDE loss还在1e-5徘徊，BC loss则时有反弹。这种不平衡正是缺乏权重调节的典型表现。

此外值得注意的是，Gemini使用了20000个PDE配点（Claude用了5000个）和1000个边界/初始条件点（Claude用了300+600）。更密的配点采样对高频问题也有帮助——频率越高，需要越多的采样点才能充分捕捉解的空间变化。

---

## L-BFGS阶段：一处实现差异，训练效率天差地别

两个模型都按照要求实现了Adam+L-BFGS两阶段训练。Adam阶段两者的思路基本一致：20000步迭代，学习率1e-3（Claude还加了StepLR衰减策略，每5000步学习率减半，这是一个不错的工程细节）。但到了L-BFGS阶段，两者的实现方式出现了根本性的分歧。

Claude的L-BFGS使用的是**外循环逐步调用**的方式：

```python
while lbfgs_step[0] < LBFGS_MAX_ITER:
    optimizer_lbfgs.step(closure)
    lbfgs_step[0] += 1
```

每一次`optimizer_lbfgs.step(closure)`都会重新调用`compute_loss`，而`compute_loss`内部每次都会**重新随机采样**配点：

```python
def compute_loss(model):
    xr, tr = sample_res(N_RES)       # 每次调用都生成新的随机配点
    # ...
    xi, ti, ui = sample_ic(N_IC)     # 每次调用都生成新的随机IC点
    # ...
```

这里有一个严重的问题。L-BFGS是拟牛顿方法，它通过近似Hessian矩阵来进行二阶优化。这类方法的核心假设之一是**目标函数在连续迭代之间保持一致**——因为它需要利用历史梯度信息来构建曲率近似。如果每一步的训练数据都在变化，等于每一步优化的是不同的目标函数，L-BFGS积累的曲率信息就失去了意义，算法退化为一种低效的伪随机优化过程。

这就解释了为什么Claude的L-BFGS阶段跑了**1795秒**（将近30分钟），10000步迭代完成后L2误差仅从1.17e-3缓慢降到4.83e-4。从loss_history数据可以清楚地看到，L-BFGS阶段的loss曲线在4e-6附近反复震荡，几乎没有持续下降的趋势——这正是随机采样破坏了L-BFGS收敛性的典型症状。

Gemini的做法则截然不同。它在训练开始前就固定了所有配点数据（`X_f`、`X_0`、`X_b`都是一次性生成的固定张量），然后L-BFGS通过单次调用`optimizer_lbfgs.step(closure)`完成内部迭代。在确定性目标函数上，L-BFGS的二阶收敛优势得以充分发挥，整个L-BFGS阶段仅耗时约**20秒**，L2误差就从2.24e-3快速降到了3.93e-4以下。

| 阶段 | Claude Sonnet 4.6 | Gemini 3.1 |
|------|-------------------|------------|
| Adam 20000步 | 308s, L2=1.17e-3 | ~220s, L2=2.24e-3 |
| L-BFGS | 1795s, L2=4.83e-4 | ~20s, L2≈3.93e-4 |
| **总计** | **2105s（35分钟）** | **~239s（4分钟）** |
| **最终L2相对误差** | **4.83×10⁻⁴** | **≈3.93×10⁻⁴** |

总训练时间差了将近**9倍**，最终精度Gemini反而略优。当然，这两个数值都远超提示词要求的1e-2精度门槛，但效率层面的差距是碾压级的。

---

## 代码工程与可视化：各有千秋

如果只看最终精度和速度，这场对比似乎是一边倒。但公平地说，Claude Sonnet 4.6在代码工程层面展现出的素养是值得肯定的。

Claude的代码共534行，结构严谨，划分为配置区、网络定义、数据采样、训练循环、数据保存、可视化等清晰的模块，每个模块都有ASCII横线分隔和注释。它输出了12个文件：包含阶段标记的loss历史（支持按Adam/L-BFGS分别分析）、预测值/精确解/误差矩阵的独立txt文件、模型权重pth文件，以及一份自动生成的`summary.txt`汇总报告。可视化方面，Claude生成了5张图——六面板loss仪表盘、三个时间切片对比、2D等高线、3D曲面，还有一张独立的L2收敛曲线，配色和布局都很专业。

Gemini的代码308行，精简高效。它没有Claude那样细致的模块划分和注释，数据保存也相对简单（loss历史、解场数据、误差数据各一个txt）。可视化输出3张图：一张双Y轴的loss+L2联合曲线（这个设计很实用，一眼就能看到loss各分量和L2误差的对应关系），一张精确解/预测/误差的三栏对比，一张时间切片。图表数量少一些，但够用。

一个有意思的细节：Gemini的loss曲线采用了双Y轴设计，左轴是loss各分量（log scale），右轴是L2 relative error（log scale），中间用红色竖线标注Adam到L-BFGS的切换点。这个设计在科研论文中很常见——审稿人最关心的就是"loss降的同时L2也在降吗"，一张图回答两个问题。Claude则把loss和L2放在不同的子图中，信息完整但需要来回对照。

总体而言，Claude交出的是一份"可以直接放进项目文档"的工程化代码，而Gemini交出的是一份"可以直接跑出论文级结果"的学术化代码。对于科研人员来说，后者的优先级显然更高。

---

## 公正性审查：是否存在"作弊"嫌疑？

做对比实验，公正性是底线。在得出结论之前，我专门对两份代码做了一次严格的"作弊审查"，逐一排查精确解泄露、训练-测试数据混用、评估指标不等价等常见陷阱。

先说结论：**两份代码都不存在作弊行为**，训练过程均未使用精确解信息。两者的PDE残差loss都只用了热方程本身（$u_t - \alpha u_{xx} = 0$），初始条件标签来自 $u(x,0)=\sin(k\pi x)$ 这一已知条件（这是问题的定义，不是"偷看答案"），边界条件则直接给定为零。精确解 $u(x,t)=\sin(k\pi x)\exp(-\alpha(k\pi)^2 t)$ 仅在训练结束后用于计算L2相对误差，不参与任何梯度计算和参数更新。

不过，审查过程中我发现了三个值得说明的方法论差异，对结果的公平解读有直接影响。

**第一，计算域不同**。Claude在 $[0,1]\times[0,1]$ 上求解，Gemini在 $[-1,1]\times[0,1]$ 上求解。这意味着什么？$\sin(5\pi x)$ 在 $[0,1]$ 上有2.5个完整振荡周期，而在 $[-1,1]$ 上有5个完整振荡周期——Gemini实际求解的问题包含了**两倍的空间振荡数**。换句话说，Gemini给自己选了一道更难的题目，却依然跑出了更优的精度和更快的速度。这一点如果不仔细看代码很容易忽略，但它让Gemini的结果更加令人信服而非相反。

**第二，L2误差指标的等价性**。Claude用的是 $\sqrt{\text{mean}(e^2)} / (\sqrt{\text{mean}(u_{\text{exact}}^2)} + 10^{-10})$，Gemini用的是 $\|e\|_2 / \|u_{\text{exact}}\|_2$。初看公式不一样，但仔细推算就会发现：前者分子分母各除以 $\sqrt{N}$（$N$ 为测试点数），比值中 $\sqrt{N}$ 恰好约去，两个指标在数学上是等价的（Claude加的 $10^{-10}$ 偏移量相对精确解的范数可以忽略不计）。虽然两者测试网格密度不同（Claude用100×100，Gemini用512×200），但作为相对误差指标，网格密度只要足够密就不影响比值的收敛性。两个L2数值可以直接横向比较。

**第三，L-BFGS的"1万次"是否真的跑满了？** 这一点需要仔细辨析。Claude的外循环跑了整整10000步，每步内部最多做20次line search迭代，实际计算量远超"1万次"。而Gemini用单次`optimizer_lbfgs.step(closure)`调用并设置`max_iter=10000`，但由于同时设定了收敛容差（`tolerance_grad=1e-7`、`tolerance_change=1e-9`），L-BFGS在检测到梯度和目标函数变化量足够小时会自动终止。从训练日志看，Gemini的closure被调用了约1000次后L-BFGS就判定收敛退出了。严格来说，Gemini并没有跑满"1万次L-BFGS"，而是**优化器自行判定了收敛**。这不是作弊，但确实说明Gemini的L-BFGS实现更符合该算法的设计初衷——达到收敛条件就停止，而不是机械地跑完预设步数。反观Claude的实现，由于随机重采样导致目标函数持续波动，L-BFGS的收敛判定永远无法触发，只能硬跑完10000步外循环，耗时却不见显著精度提升。

综合以上分析，这次对比的公正性是经得起推敲的。两份代码的训练数据来源合法，评价指标等价，Gemini甚至在更大的计算域上求解了更困难的问题。性能差距的根源确实在于算法选择和实现策略的差异，而非任何形式的"投机取巧"。

---

## 复盘与思考

回头看这次测试，最核心的发现在于：面对同一句提示词，两个模型对"高频PDE"这个科研任务的理解深度截然不同。

Gemini 3.1表现出了一种可以称之为"学术直觉"的特质——它准确地识别出高频问题中频谱偏置是核心瓶颈，主动引入了傅里叶特征映射；它清楚PINNs的多任务损失需要加权平衡，给出了合理的IC/BC权重倍数；它正确地将L-BFGS与固定数据配合使用，让二阶优化器的收敛性得到保障。这三个决策环环相扣，共同造就了4分钟跑出3.93e-4的高效结果。

Claude Sonnet 4.6则表现出了一种"扎实但保守"的风格——代码工程质量过硬，注释详尽，输出完备，但在算法选择上缺乏对任务特殊性的响应。标准MLP加等权重损失加随机重采样L-BFGS，每一个单独拎出来都不算错，但组合在一起面对高频问题时，效率和效果就打了折扣。

对我而言，这次测试揭示了一个有启发性的观察：**在科研编程场景中，模型的学科知识储备和问题理解能力，可能比纯粹的代码生成能力更为关键**。写出没有bug的代码是基本功，但选对算法、调对超参、用对技术——这才是科研编程真正难的地方。至少在PINN求解高频PDE这个任务上，Gemini 3.1展现出了更强的科研场景适配能力。

当然，一个测试案例不能代表全部。Claude在代码规范性、输出完整性和工程可维护性上的优势也是客观存在的。未来如果有机会，我会在更多的科研场景下继续对比——比如反问题、多物理场耦合、高维PDE等，看看这种差异是否具有普遍性。

---

*测试环境：NVIDIA GPU + PyTorch，提示词完全相同，两个模型均一次生成，未经人工修改直接运行。*

*Gemini 3.1 Pro（2026.02.19发布）| Claude Sonnet 4.6（2026.02.17发布）*
